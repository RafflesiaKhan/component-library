{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-delta",
   "metadata": {
    "papermill": {
     "duration": 0.016304,
     "end_time": "2021-03-22T20:29:23.476444",
     "exception": false,
     "start_time": "2021-03-22T20:29:23.460140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# anomaly_score_unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97e294-9399-4d96-a95c-8ad7e29a2872",
   "metadata": {},
   "source": [
    "Send arbitrary time-series data to the component and train an unsupervised LSTM-Autoencoder model. The moment unseen patters occur the anomaly score rises.\n",
    "\n",
    "Future work:\n",
    "\n",
    "- reset / rollback model (for regular flushing or after a real anomaly (true positive) occurred)\n",
    "- add check-pointing for service persistence and rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884bff3a-dc51-4c8b-a98f-1d0a8ac1de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ['create_image']='True'\n",
    "#os.environ['repository']='romeokienzler'\n",
    "#os.environ['version']='0.1'\n",
    "#\n",
    "os.environ['install_requirements']='False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f19a1a1-9cf2-4cb6-a0d0-859c9de3a525",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.9.1 in /opt/conda/lib/python3.9/site-packages (2.9.1)\n",
      "Requirement already satisfied: numpy==1.23.2 in /opt/conda/lib/python3.9/site-packages (1.23.2)\n",
      "Requirement already satisfied: scikit-learn==1.1.2 in /opt/conda/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: pandas==1.4.3 in /opt/conda/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: flask==2.2.2 in /opt/conda/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (4.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.12)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.7.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.19.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (21.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.1.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (14.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.47.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (0.26.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (63.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.9.1) (0.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.1.2) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.1.2) (1.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.1.2) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas==1.4.3) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas==1.4.3) (2.8.2)\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/lib/python3.9/site-packages (from flask==2.2.2) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.9/site-packages (from flask==2.2.2) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.9/site-packages (from flask==2.2.2) (4.8.1)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.9/site-packages (from flask==2.2.2) (2.2.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.9/site-packages (from flask==2.2.2) (2.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask==2.2.2) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0->flask==2.2.2) (2.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow==2.9.1) (2.4.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "if bool(os.environ.get('create_image',False)):\n",
    "    docker_file=\"\"\"\n",
    "    FROM registry.access.redhat.com/ubi8/python-39\n",
    "    RUN pip install ipython nbformat numpy ibm-cos-sdk-core ibm-cos-sdk ibm-watson-machine-learning ibm-watson-studio-pipelines ibmcloudsql pyyaml\n",
    "    ADD ibm-sql-query-cpd.py .\n",
    "    ADD start.sh .\n",
    "\n",
    "    \"\"\"\n",
    "    with open(\"Dockerfile\", \"w\") as text_file:\n",
    "        text_file.write(docker_file)\n",
    "\n",
    "    start_file=\"\"\"\n",
    "    #!/bin/bash\n",
    "    echo \"Parameter 1: $1\"\n",
    "    echo \"Parameter 2: $2\"\n",
    "    echo \"Parameter 3: $3\"\n",
    "    echo \"Parameter 4: $4\"\n",
    "    echo \"Parameter 5: $5\"\n",
    "    echo \"Parameter 6: $6\"\n",
    "    echo \"Parameter 7: $7\"\n",
    "    echo \"Parameter 8: $8\"\n",
    "    echo \"Parameter 9: $9\"\n",
    "    echo \"Parameter 10: ${10}\"\n",
    "    echo \"Parameter 11: ${11}\"\n",
    "    echo \"Parameter 12: ${12}\"\n",
    "    echo \"Parameter 13: ${13}\"\n",
    "    echo \"Parameter 14: ${14}\"\n",
    "    echo \"Parameter 15: ${15}\"\n",
    "    echo \"Parameter 16: ${16}\"\n",
    "    echo \"Parameter 17: ${17}\"\n",
    "    echo \"Parameter 18: ${18}\"\n",
    "    echo \"Parameter 19: ${19}\"\n",
    "    echo \"Parameter 20: ${20}\"\n",
    "    python /opt/app-root/src/ibm-sql-query-cpd.py \"$1$2\" \"$3$4\" \"$5$6\" \"$7$8\" \"$9${10}\" \"${11}${12}\" \"${13}${14}\" \"${15}${16}\" \"${17}${18}\" \"${19}${20}\"\n",
    "    \"\"\"\n",
    "    with open(\"start.sh\", \"w\") as text_file:\n",
    "        text_file.write(start_file)\n",
    "\n",
    "    !chmod 755 start.sh\n",
    "    !jupyter nbconvert --to script ibm-sql-query-cpd.ipynb    \n",
    "    !docker build -t ibm_sql_query_cpd:`echo $version` .\n",
    "    !docker tag ibm_sql_query_cpd:`echo $version` `echo $repository`/ibm_sql_query_cpd:`echo $version`\n",
    "    !docker push `echo $repository`/ibm_sql_query_cpd:`echo $version`\n",
    "    !rm Dockerfile\n",
    "    !rm ibm-sql-query-cpd.py\n",
    "    !rm start.sh\n",
    "elif bool(os.environ.get('install_requirements',False)):\n",
    "    !pip install tensorflow==2.9.1 numpy==1.23.2 scikit-learn==1.1.2  pandas==1.4.3 flask==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e63f153-5524-4c3a-8b55-2afb01f2750b",
   "metadata": {
    "papermill": {
     "duration": 0.164002,
     "end_time": "2021-03-22T20:29:25.951504",
     "exception": false,
     "start_time": "2021-03-22T20:29:25.787502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 14:07:06.682681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-22 14:07:06.682726: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "\n",
    "from flask import request\n",
    "from flask import Flask\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abstract-cambridge",
   "metadata": {
    "papermill": {
     "duration": 0.012801,
     "end_time": "2021-03-22T20:29:25.972462",
     "exception": false,
     "start_time": "2021-03-22T20:29:25.959661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COS URL where the results of the SQL job are to be stored\n",
    "target_dir_path = os.environ.get('target_dir_path')\n",
    "\n",
    "# Asset name to register for the results written by the SQL job\n",
    "target_asset_name = os.environ.get('target_asset_name')\n",
    "\n",
    "# sql statement to execute\n",
    "sql = os.environ.get('sql')\n",
    "\n",
    "# (unique) Custom Resource Name (CRN) of IBM SQL Query Service\n",
    "data_engine_crn = os.environ.get('data_engine_crn')\n",
    "\n",
    "# default: CSV - (will be generated into according STORED AS … clause in the INTO clause)\n",
    "format = os.environ.get('format' , 'CSV')\n",
    "\n",
    "# optional, list of columns to use for partitioning the results of the SQL job, will be generated into according PARTITIONED BY (<columns>) clause in the INTO clause)\n",
    "partition_columns = os.environ.get('partition_columns','')\n",
    "\n",
    "# optional, number of objects to store the results of the SQL job in, will be generated into according PARTITIONED INTO <num> OBJECTS clause in INTO clause\n",
    "number_of_objects = int(os.environ.get('number_of_objects', 0))\n",
    "\n",
    "# optional, number of rows to be stored in each result object of the SQL job, will be generated into according PARTITIONED EVERY <num> ROWS clause in INTO clause\n",
    "rows_per_object = int(os.environ.get('rows_per_object', 0))\n",
    "\n",
    "# default: False, only valid when none of the above partitioning option is specified, produces exactly one object with name specified in target_dir_path, twill be generated into sqlClient.rename_exact_result(jobid) after SQL has run.\n",
    "exact_name = os.environ.get('exact_name', 'False')\n",
    "\n",
    "# default: False - will be generated into JOBPREFIX NONE in the INTO clause. Will cause results of previous runs with same output_uri to be overwritten, because no unique sub folder will be created for the result)\n",
    "no_jobid_folder = os.environ.get('no_jobid_folder', 'False')\n",
    "\n",
    "# default: output.txt - output file name containing the CPD path of the resulting asset\n",
    "data_asset = os.environ.get('data_asset','output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f9b39f-2c8c-4ab5-b4f5-513357bf20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:argv raw /opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\n",
      "WARNING:root:argv raw -f\n",
      "WARNING:root:argv raw /home/jovyan/.local/share/jupyter/runtime/kernel-c1a2870e-85c0-4654-9dc6-aeb8a3bac479.json\n"
     ]
    }
   ],
   "source": [
    "for element in sys.argv:\n",
    "    logging.warning('argv raw ' +  element)\n",
    "\n",
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "\n",
    "for parameter in parameters:\n",
    "    exec(parameter)\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "\n",
    "\n",
    "for parameter in parameters:\n",
    "    exec(\"logging.warning('final parameter: ' + str({}))\".format(parameter.split('=')[0]))\n",
    "    exec(\"logging.warning('final parameter type: ' + str(type({})))\".format(parameter.split('=')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a93042-b9fc-417c-9eea-7b2715219616",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./anomaly-score-unsupervised/watsoniotp.healthy.phase_aligned.pickle','rb') as file_object:\n",
    "    raw_data = file_object.read()\n",
    "    data_healthy = pickle.loads(raw_data, encoding='latin1')\n",
    "\n",
    "with open('./anomaly-score-unsupervised/watsoniotp.broken.phase_aligned.pickle','rb') as file_object:\n",
    "    raw_data = file_object.read()\n",
    "    data_broken = pickle.loads(raw_data, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ad728c-70aa-41b6-9885-40d3b32708f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy = data_healthy.reshape(3000,3)\n",
    "data_broken = data_broken.reshape(3000,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d85be76-c42f-4967-9525-5c273ca4efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0315b9fc-14bb-4e14-ac77-08695ef12059",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_scaled = scaleData(data_healthy)\n",
    "data_broken_scaled = scaleData(data_broken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f4985f-715b-4ee2-838f-c76056ecc520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "y = np.array([10,11,12,13,14,15,16,17,18,19])\n",
    "z = np.array([20,21,22,23,24,25,26,27,28,29])\n",
    "data = np.vstack((x,y,z))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0dbf8ef-c486-4c66-8e82-0ccb5b26609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_data_transform(x_data, y_data, num_steps=5):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()    # Loop of the entire data set\n",
    "    for i in range(x_data.shape[0]):\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + num_steps        # if index is larger than the size of the dataset, we stop\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break        # Get a sequence of data for x\n",
    "        seq_X = x_data[i:end_ix]\n",
    "        # Get only the last element of the sequency for y\n",
    "        seq_y = y_data[end_ix]        # Append the list with sequencies\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91ffde5-2f55-4313-b900-6617e40416d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = lstm_data_transform(data,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a770bb-9576-415d-af4a-2218bd9f3e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]],\n",
       "\n",
       "       [[15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape = (2,3,5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b3c3ad-435c-47af-84a1-7f5949aa9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10\n",
    "dim = 3\n",
    "samples = 3000\n",
    "data_healthy_scaled_reshaped = data_healthy_scaled\n",
    "#reshape to (300,10,3)\n",
    "data_healthy_scaled_reshaped.shape = (int(samples/timesteps),timesteps,dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75983ac5-9abd-4d01-b864-8015b18e5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        loss_history = [] \n",
    "\n",
    "    def on_train_batch_end(self, batch, logs):\n",
    "        print('Loss on_train_batch_end '+str(logs.get('loss')))\n",
    "        loss_history.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da08143-9d23-4992-8069-c69b45914142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 14:07:09.458958: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-22 14:07:09.458993: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-22 14:07:09.459019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (elyra): /proc/driver/nvidia/version does not exist\n",
      "2022-08-22 14:07:09.459292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(3))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    data.shape = (300, 10, 3)\n",
    "    model.fit(data, data, epochs=3, batch_size=72, validation_data=(data, data), verbose=0, shuffle=False,callbacks=[LossHistory()])\n",
    "    data.shape = (3000, 3)\n",
    "\n",
    "def score(data):\n",
    "    data.shape = (300, 10, 3)\n",
    "    yhat =  model.predict(data)\n",
    "    yhat.shape = (3000, 3)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a0062d-278e-414d-a838-e0783dc0e269",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(0):\\n\\n    print(\"----------------\")\\n    train(data_healthy_scaled)\\n    yhat_healthy = score(data_healthy_scaled)\\n    yhat_broken = score(data_broken_scaled)\\n    data_healthy_scaled.shape = (3000, 3)\\n    data_broken_scaled.shape = (3000, 3)\\n\\nprint(\"----------------broken\")\\ntrain(data_broken_scaled)\\nyhat_healthy = score(data_healthy_scaled)\\nyhat_broken = score(data_broken_scaled)\\ndata_healthy_scaled.shape = (3000, 3)\\ndata_broken_scaled.shape = (3000, 3)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for i in range(0):\n",
    "\n",
    "    print(\"----------------\")\n",
    "    train(data_healthy_scaled)\n",
    "    yhat_healthy = score(data_healthy_scaled)\n",
    "    yhat_broken = score(data_broken_scaled)\n",
    "    data_healthy_scaled.shape = (3000, 3)\n",
    "    data_broken_scaled.shape = (3000, 3)\n",
    "\n",
    "print(\"----------------broken\")\n",
    "train(data_broken_scaled)\n",
    "yhat_healthy = score(data_healthy_scaled)\n",
    "yhat_broken = score(data_broken_scaled)\n",
    "data_healthy_scaled.shape = (3000, 3)\n",
    "data_broken_scaled.shape = (3000, 3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ec81c3b-a41f-46ec-8881-a2aa774da90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNN(data):\n",
    "    data_scaled = scaleData(data)\n",
    "    train(data_scaled)\n",
    "    yhat = score(data_scaled)\n",
    "    data_scaled.shape = (3000, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56724c-033f-45ac-9765-9265c559b078",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://10.244.0.22:8080\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.08        3.37536     3.97083266]\n",
      " [ 2.1836288   3.76812337  3.95194702]\n",
      " [ 2.31038837  4.18246112  3.9449437 ]\n",
      " ...\n",
      " [ 4.67227426  8.55622075 11.7405177 ]\n",
      " [ 4.98298998  9.13593768 11.85424761]\n",
      " [ 5.31522579  9.74939673 12.01591895]]\n",
      "Loss on_train_batch_end 0.3888910114765167\n",
      "Loss on_train_batch_end 0.4272225499153137\n",
      "Loss on_train_batch_end 0.44438934326171875\n",
      "Loss on_train_batch_end 0.44490236043930054\n",
      "Loss on_train_batch_end 0.447885125875473\n",
      "Loss on_train_batch_end 0.34840255975723267\n",
      "Loss on_train_batch_end 0.3837062120437622\n",
      "Loss on_train_batch_end 0.3976879119873047\n",
      "Loss on_train_batch_end 0.394283652305603\n",
      "Loss on_train_batch_end 0.3962740898132324\n",
      "Loss on_train_batch_end 0.26323315501213074\n",
      "Loss on_train_batch_end 0.29454872012138367\n",
      "Loss on_train_batch_end 0.30323049426078796\n",
      "Loss on_train_batch_end 0.2998073101043701\n",
      "Loss on_train_batch_end 0.30089956521987915\n",
      "10/10 [==============================] - 6s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [22/Aug/2022 14:08:27] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def index():\n",
    "    message = request.get_json()\n",
    "    #message = message[1:-1] # get rid of encapsulating quotes\n",
    "    #json_array = json.loads()\n",
    "    data = numpy.asarray(message)\n",
    "    print(data)\n",
    "    doNN(data)\n",
    "    return json.dumps(loss_history)\n",
    "\n",
    "app.run(host='0.0.0.0', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 470.538548,
   "end_time": "2021-03-22T20:37:13.369954",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/examples/pipelines/pairs/component-library/transform/spark-csv-to-parquet.ipynb",
   "output_path": "/home/jovyan/work/examples/pipelines/pairs/component-library/transform/spark-csv-to-parquet.ipynb",
   "parameters": {},
   "start_time": "2021-03-22T20:29:22.831406",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
